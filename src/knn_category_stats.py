from __future__ import annotations

import argparse
from pathlib import Path

import numpy as np
import pandas as pd


REQUIRED_COLUMNS = {"query_id", "neighbor_id"}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Compute per-category top-k neighbor category-match statistics from search_all outputs"
    )
    parser.add_argument(
        "--knn_dir",
        type=Path,
        required=True,
        help="Root directory generated by src.search_all (contains per-query knn_*.csv)",
    )
    parser.add_argument(
        "--out",
        type=Path,
        required=True,
        help="Output CSV path for per-category summary stats",
    )
    parser.add_argument(
        "--per_query_out",
        type=Path,
        default=None,
        help="Optional output CSV path for per-query match-rate rows",
    )
    parser.add_argument(
        "--glob",
        type=str,
        default="**/knn_*.csv",
        help="Glob pattern under --knn_dir to locate per-query CSV files",
    )
    return parser.parse_args()


def category_of(specimen_id: str) -> str:
    return specimen_id.split("/", 1)[0]


def load_per_query_rates(knn_dir: Path, pattern: str) -> pd.DataFrame:
    csv_paths = sorted(knn_dir.glob(pattern))
    if not csv_paths:
        raise FileNotFoundError(f"No CSV files found under {knn_dir} with pattern: {pattern}")

    rows: list[dict[str, float | str | int]] = []
    for csv_path in csv_paths:
        df = pd.read_csv(csv_path)
        if not REQUIRED_COLUMNS.issubset(df.columns):
            missing = REQUIRED_COLUMNS - set(df.columns)
            raise ValueError(f"{csv_path} is missing required columns: {sorted(missing)}")
        if df.empty:
            continue

        query_id = str(df.iloc[0]["query_id"])
        query_category = category_of(query_id)

        neighbor_categories = df["neighbor_id"].astype(str).map(category_of)
        is_match = (neighbor_categories == query_category).astype(np.int32)

        rows.append(
            {
                "query_id": query_id,
                "category": query_category,
                "topk": int(len(df)),
                "match_count": int(is_match.sum()),
                "match_rate": float(is_match.mean()),
            }
        )

    if not rows:
        raise ValueError("No non-empty kNN CSV rows were found.")

    return pd.DataFrame(rows)


def summarize_by_category(per_query: pd.DataFrame) -> pd.DataFrame:
    grouped = per_query.groupby("category", as_index=False)["match_rate"]
    summary = grouped.agg(
        n_queries="count",
        mean_match_rate="mean",
        std_match_rate="std",
        var_match_rate="var",
        min_match_rate="min",
        q25_match_rate=lambda s: s.quantile(0.25),
        median_match_rate="median",
        q75_match_rate=lambda s: s.quantile(0.75),
        max_match_rate="max",
    )
    summary["std_match_rate"] = summary["std_match_rate"].fillna(0.0)
    summary["var_match_rate"] = summary["var_match_rate"].fillna(0.0)

    summary["mean_match_percent"] = summary["mean_match_rate"] * 100.0
    summary["std_match_percent"] = summary["std_match_rate"] * 100.0

    return summary.sort_values("mean_match_rate", ascending=False).reset_index(drop=True)


def main() -> None:
    args = parse_args()

    per_query = load_per_query_rates(args.knn_dir, args.glob)
    summary = summarize_by_category(per_query)

    args.out.parent.mkdir(parents=True, exist_ok=True)
    summary.to_csv(args.out, index=False)

    if args.per_query_out is not None:
        args.per_query_out.parent.mkdir(parents=True, exist_ok=True)
        per_query.sort_values(["category", "query_id"]).to_csv(args.per_query_out, index=False)

    print(f"Saved category summary: {args.out}")
    print(f"Categories: {len(summary)} | Queries: {len(per_query)}")
    if args.per_query_out is not None:
        print(f"Saved per-query stats: {args.per_query_out}")


if __name__ == "__main__":
    main()
